{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.   Load Dataset\n",
    "\n",
    "**[2.0]** Launch the magic commands for auto-relaoding external modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Launch the magic commands for auto-relaoding external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2.1]** Import the torch and torchvision packages\n",
    "\n",
    "**[2.2]** Create a variable called `download` containing the value `True`\n",
    "\n",
    "**[2.3]** Define a transformation pipeline that will convert the images into tensors and normalise them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the torch and torchvision packages\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Create a variable called download containing the value True\n",
    "download = True\n",
    "\n",
    "# Define a transformation pipeline that will convert the images into tensors and normalise them\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2.4]** Instantiate a torchvision.datasets.MNIST() for the training set, downlows it into `/data/raw/` folder and perform the transformation defined earlier. Save the results in a variable called `train_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a torchvision.datasets.MNIST() for the training set, downlows it into /data/raw/ folder and perform the transformation defined earlier. Save the results in a variable called train_data\n",
    "train_data = torchvision.datasets.MNIST('../data/raw/', train=True, download=download, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2.5]** Instantiate a torchvision.datasets.MNIST() for the testing set, downlows it into `/data/raw/` folder and perform the transformation defined earlier. Save the results in a variable called `test_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a torchvision.datasets.MNIST() for the testing set, downlows it into /data/raw/ folder and perform the transformation defined earlier. Save the results in a variable called test_data\n",
    "test_data = torchvision.datasets.MNIST('../data/raw/', train=False, download=download, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Data\n",
    "\n",
    "**[3.1]** Create 2 variables called `batch_size_train` and `batch_size_test` that will respectively take the values 64 and 10\n",
    "\n",
    "**[3.2]** Import DataLoader from torch.utils.data\n",
    "\n",
    "**[3.3]** Instantiate a `torch.utils.data.DataLoader()` on the training data, with the relevant batch size and with shuffle. Save the reults in a variable called `train_loader`\n",
    "\n",
    "**[3.4]** Instantiate a `torch.utils.data.DataLoader()` on the testing data, with the relevant batch size and with shuffle. Save the reults in a variable called `test_loader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 variables called batch_size_train and batch_size_test that will respectively take the values 64 and 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 10\n",
    "\n",
    "# Import DataLoader from torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Instantiate a torch.utils.data.DataLoader() on the training data, with the relevant batch size and with shuffle. Save the reults in a variable called train_loader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# Instantiate a torch.utils.data.DataLoader() on the testing data, with the relevant batch size and with shuffle. Save the reults in a variable called test_loader\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.5]** Create a generator on the test data loader and extract the first observation\n",
    "\n",
    "**[3.6]** Print the dimensions of the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a generator on the test data loader and extract the first observation\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "# Print the dimensions of the first image\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3.7]** Import matplotlib.pyplot as plt\n",
    "\n",
    "**[3.8]** Print the first image with its corresponding target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ground Truth: 7')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print the first image with its corresponding target\n",
    "plt.imshow(example_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.title(\"Ground Truth: {}\".format(example_targets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Define Architecture\n",
    "\n",
    "**[4.1]** Create a class called `PytorchCNN` that inherits from `nn.Module` with:\n",
    "- attributes:\n",
    "    - `conv1`: fully-connected layer with 128 filters of size 3\n",
    "    - `conv2`: fully-connected layer with 64 filters of size 3\n",
    "    - `fc1`: fully-connected layer with 128 neurons\n",
    "    - `fc2`: fully-connected layer with 10 neurons\n",
    "    - `softmax`: Softmax activation function\n",
    "- methods:\n",
    "    - `forward()` with `inputs` as input parameter and will sequentially add the 2 convolution layers with relu and max pool of size 2 followed the 2 full-connected layers respectively with relu and softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4.2]** Import torch.nn as n, torch.nn.functional as F and torch.optim as optim\n",
    "\n",
    "**[4.3]** Instantiate a PytorchCNN and save it into a variable called `model` \n",
    "\n",
    "**[4.4]** Import the `get_device` function from src.models.pytorch \n",
    "\n",
    "**[4.5]** Get the device available and set to the model to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PytorchCNN(\n",
       "  (conv1): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch.nn as n, torch.nn.functional as F and torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Import PytorchCNN() and instantiate a PytorchCNN and save it into a variable called model\n",
    "from src.models.pytorch import PytorchCNN\n",
    "model = PytorchCNN()\n",
    "\n",
    "# Import the get_device function from src.models.pytorch\n",
    "from src.models.pytorch import get_device\n",
    "\n",
    "# Get the device available and set to the model to use it\n",
    "device = get_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the model\n",
    "\n",
    "**[5.1]** Import train_classification and test_classification from src.models.pytorch\n",
    "\n",
    "**[5.2]** Instantiate a `nn.CrossEntropyLoss()` and save it into a variable called `criterion`\n",
    "\n",
    "**[5.3]** Instantiate a torch.optim.Adam() optimizer with the model's parameters and 0.001 as learning rate and save it into a variable called optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import train_classification and test_classification from src.models.pytorch\n",
    "from src.models.pytorch import train_classification, test_classification\n",
    "\n",
    "# Instantiate a nn.CrossEntropyLoss() and save it into a variable called criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate a torch.optim.Adam() optimizer with the model's parameters and 0.001 as learning rate \n",
    "# and save it into a variable called optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5.4]** Create 2 variables called `N_EPOCHS` and `BATCH_SIZE` that will respectively take the values 50 and 32\n",
    "\n",
    "**[5.5]** Create a for loop that will iterate through the specified number of epochs and will train the model with the training set and assess the performance on the validation set and print their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t(train)\t|\tLoss: 0.0490\t|\tAcc: 89.4%\n",
      "\t(valid)\t|\tLoss: 0.0465\t|\tAcc: 97.4%\n",
      "Epoch: 1\n",
      "\t(train)\t|\tLoss: 0.0465\t|\tAcc: 97.4%\n",
      "\t(valid)\t|\tLoss: 0.0464\t|\tAcc: 97.7%\n",
      "Epoch: 2\n",
      "\t(train)\t|\tLoss: 0.0463\t|\tAcc: 97.8%\n",
      "\t(valid)\t|\tLoss: 0.0468\t|\tAcc: 96.5%\n"
     ]
    }
   ],
   "source": [
    "# Variables N_EPOCHS and BATCH_SIZE that will respectively take the values 50 and 32:\n",
    "N_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create a for loop that will iterate through the specified number of epochs and will train the model \n",
    "# with the training set and assess the performance on the validation set and print their scores\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train_classification(train_data, model=model, criterion=criterion, optimizer=optimizer, batch_size=BATCH_SIZE, device=device)\n",
    "    valid_loss, valid_acc = test_classification(test_data, model=model, criterion=criterion, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\t(train)\\t|\\tLoss: {train_loss:.4f}\\t|\\tAcc: {train_acc * 100:.1f}%')\n",
    "    print(f'\\t(valid)\\t|\\tLoss: {valid_loss:.4f}\\t|\\tAcc: {valid_acc * 100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[5.6]** Save the model into the `models` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model into the models folder\n",
    "torch.save(model, \"../models/pytorch_mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.   Push changes\n",
    "\n",
    "**[6.8]** Then go to Github and merge the branch after reviewing the code and fixing any conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Add you changes to git staging area\n",
    "git add .\n",
    "\n",
    "# Create the snapshot of your repository and add a description\n",
    "git commit -m \"pytorch cnn mnist\"\n",
    "\n",
    "# Push your snapshot to Github\n",
    "git push https://<insert_pat>@github.com/CazMayhem/adv_dsi_lab_6.git\n",
    "\n",
    "# Check out to the master branch\n",
    "git checkout master\n",
    "\n",
    "# Pull the latest updates\n",
    "git pull https://<insert_pat>@github.com/CazMayhem/adv_dsi_lab_5.git\n",
    "\n",
    "# Check out to the pytorch_mnist branch\n",
    "git checkout pytorch_mnist\n",
    "\n",
    "# erge the master branch and push your changes\n",
    "git merge master\n",
    "git push https://<insert_pat>@github.com/CazMayhem/adv_dsi_lab_6.git\n",
    "\n",
    "# Now go to Github and merge the branch after reviewing the code and fixing any conflict\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6.9]** Stop the Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Docker container\n",
    "docker stop adv_dsi_lab_6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
